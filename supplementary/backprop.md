# Backpropaganda

Обратное распространение ошибки -- это рекурсивное применение производной сложной функции в вычислительном графе. Сейчас это наиболее распространенный способ вычисления производных в нейронных сетях.

На практике, рекурсия заменяется обходом графа в правильном порядке, а вычисления векторизуются, forward и backward pass делаются не отдельными семплами, а мини-батчами.

## Математическая сторона вопроса

Пусть вычислительный граф состоит из _узлов_.  Узел имеет несколько входов ($x_{i}$) и один выход ($z$), внутри он применяет некоторую функцию $f$. Т.е. во время forward-прохода вычисляется $z = f(x_1, x_2, ... x_n)$.

Где-то дальше в вычислительном графе вычисляется скаляр L -- величина ошибки. Наша задача вычислить $\frac{\partial L}{\partial x_{i}}$.

1. **База индукции.** Ошибка (loss) $L$ вычисляется в последнем узле по известной нам аналитической формуле, для нее мы можем записать производную. И соответственно посчитать частные производные по входам в последний узел.
2. **Шаг индукции.** В очередной узел приходит тензор $\frac{\partial L}{\partial z}$,  мы хотим вычислить тензоры $\frac{\partial L}{\partial x_i}$. Это можно сделать с помощью формулы производной сложной функции (aka **chain rule**): $\frac{\partial L}{\partial x_i} = \frac{\partial L}{\partial z} \frac{\partial z}{\partial x_i}$. Производные $\frac{\partial z}{\partial x_i}$ называются локальными производными. В общем случае, для их вычислениям нам необходимо помнить $x_i$.

В итоге пройдя по всем вершинам графа в обратном порядке (в смысле топологической сортировки) мы можем вычислить вектор градиентов $\frac{\partial L}{\partial \theta_i}$.



## Стоимость

Будем считать что данные прогоняются через граф батчами размером `bs`.

В каждом узле вычислительного графа сохраняется `[bs, ...dims]` чисел. Для экономии памяти стараются уменьшить количество узлов. А для скорости вычисления сделать по возможности матричными. 

В некоторых случаях можно уменьшить расход памяти на хранение активаций

- Relu
- ABN
- Reversible Networks
- Synthetic Gradients



## Ссылки

CS231N

Дифференцирование полносвязного слоя

## Недостатки

1. Взрывающиеся градиенты
2. Затухающие градиенты
3. Последовательность действий и затраты памяти

## Реализация в pytorch

https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec

